# Evaluating the Efficiency of Parallel Semantic Document Matching using Apache Spark
In the cloud and web based scenarios, the volume of the documents from various platforms like digital libraries, news articles, and online forums etc are increasing at quick pace. It is a challenging task to efficiently process this magnitude of documents. The complexity would be of much greater scale if the process also involves in matching the documents based on their semantics. In this thesis, we have implemented a process flow for matching the similar documents based on the semantics using Latent Dirichlet Allocation (LDA) techniques. LDA techniques model a document into a distribution of topics by discovering the latent structure of the document along with the word occurrences. The semantic similarity is calculated by the information gained from a large corpus. As various approaches focused on increasing the efficiency of the Latent Dirichlet Allocation technique using MapReduce based and MPI based distributed environments, the entire Semantic Document Matching process is implemented using components of Apache Spark framework. Apache Spark is an open source, parallel processing engine to compute large amounts of data. The primary goal of the thesis is focused on increasing the efficiency of the Semantic Document Matching process. Different experiments are performed on various sizes of a dataset in order to evaluate the Semantic Document Matching process.
