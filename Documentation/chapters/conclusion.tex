\ifgerman{\chapter{Zusammenfassung}}{\chapter{Conclusion}}
\label{conclusion}

In this thesis, we designed and implemented an approach to match documents based on their semantic similarity with in the corpus. To efficiently process the huge amount of documents, this thesis is implemented in a parallel processing environment using the Apache Spark framework. The semantic similarity between the documents is measured by discovering the topics from each document. The approach is implemented as a combination of Spark SQL, DataFrames API and machine learning components with in the Apache Spark framework. The designed and implemented approach consists of 6 stages namely pre-processing stage, count vectorization, Latent Dirichlet Allocation (LDA) modeling, Document Pair comparison, classification and evaluation stages.

\par As our primary focus is to increase the efficiency, We have evaluated the speedup and efficiency of the implemented approach using various experiments. For evaluating the efficiency, different sizes of the input dataset and the number of worker nodes are varied. As the approach is implemented in a parallel environment, the total execution time taken by the approach in obtaining the results is calculated. Based on the runtime obtained, speedup and efficiency are calculated. Further, we have improved the speedup and efficiency of the implementation approach by applying different optimization techniques. With the implemented approach, we also reduced the number of required computations  to less than \(O(n^2)\) computations for Document pair comparison. However, the implemented approach can only be applicable to match documents for macro level analysis i.e. calculating semantic similarity between documents by considering the structure of the document as a whole. Moreover, the efficiency of the implemented approach can further be improved either by adding more worker nodes in a cluster or by introducing more advanced optimization techniques.

Additionally, we calculated the effectiveness of the implementation to verify the quality of the results obtained. As the input dataset do not provide the golden data, we designed a workaround method and the golden data is generated to calculate the effectiveness by calculating precision, recall, and f-measure.